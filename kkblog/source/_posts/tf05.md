---
title: Tensorflow api整理(5)
date: 2018-03-27 16:11:32
tags: [设计模式,读书笔记,源码]
categories: [Android]
---


#### tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)

Applies exponential decay to the learning rate.
When training a model, it is often recommended to ***lower the learning rate as the training progresses***. This function applies an exponential decay function to a provided initial learning rate. It requires ***a global_step value to compute the decayed learning rate***. You can just pass a TensorFlow variable that you increment at each training step.

The function returns the decayed learning rate. It is computed as:

> decayed_learning_rate = learning_rate *
>                       decay_rate ^ (global_step / decay_steps)


If the argument staircase is True, then global_step /decay_steps is an integer division and the decayed learning rate follows a staircase function.

##### Example: decay every 100000 steps with a base of 0.96

``` python
...
global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.exponential_decay(starter_learning_rate, global_step,
                                     100000, 0.96, staircase=True)
optimizer = tf.GradientDescent(learning_rate)
# Passing global_step to minimize() will increment it at each step.
optimizer.minimize(...my loss..., global_step=global_step)
```

Args:
* learning_rate: A scalar float32 or float64 Tensor or a Python number. The initial learning rate.
* global_step: A scalar int32 or int64 Tensor or a Python number. Global step to use for the decay computation. Must not be negative.
* decay_steps: A scalar int32 or int64 Tensor or a Python number. Must be positive. See the decay computation above.
* decay_rate: A scalar float32 or float64 Tensor or a Python number. The decay rate.
* staircase: Boolean. It True decay the learning rate at discrete intervals.
* name: string. Optional name of the operation. Defaults to 'ExponentialDecay'

Returns:
A scalar Tensor of the same type as learning_rate. ***The decayed learning rate***.




#### class tf.train.ExponentialMovingAverage

Some training algorithms, such as ***GradientDescent and Momentum often benefit from maintaining a moving average of variables during optimization***. Using the moving averages for evaluations often improve results significantly.

Maintains moving averages of variables by employing and exponential decay.

When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.

The apply() method adds shadow copies of trained variables and add ops that maintain a moving average of the trained variables in their shadow copies. It is used when building the training model. The ops that maintain moving averages are typically run after each training step. The average() and average_name() methods give access to the shadow variables and their names. They are useful when building an evaluation model, or when restoring a model from a checkpoint file. They help use the moving averages in place of the last trained values for evaluations.

The moving averages are computed using exponential decay. You specify the decay value when creating the ExponentialMovingAverage object. The shadow variables are initialized with the same initial values as the trained variables. When you run the ops to maintain the moving averages, each shadow variable is updated with the formula:

shadow_variable -= (1 - decay) * (shadow_variable - variable)

This is mathematically equivalent to the classic formula below, but the use of an assign_sub op (the "-=" in the formula) allows concurrent lockless updates to the variables:

shadow_variable = decay * shadow_variable + (1 - decay) * variable

Reasonable values for decay are close to 1.0, typically in the multiple-nines range: 0.999, 0.9999, etc.

Example usage when creating a training model:

``` python
# Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...
...
# Create an op that applies the optimizer.  This is what we usually
# would use as a training op.
opt_op = opt.minimize(my_loss, [var0, var1])

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

# Create the shadow variables, and add ops to maintain moving averages
# of var0 and var1.
maintain_averages_op = ema.apply([var0, var1])

# Create an op that will update the moving averages after each training
# step.  This is what we will use in place of the usuall trainig op.
with tf.control_dependencies([opt_op]):
    training_op = tf.group(maintain_averages_op)

...train the model by running training_op...
```


There are two ways to use the moving averages for evaluations:

Build a model that uses the shadow variables instead of the variables. For this, use the average() method which returns the shadow variable for a given variable.
Build a model normally but load the checkpoint files to evaluate by using the shadow variable names. For this use the average_name() method. See the Saver class for more information on restoring saved variables.